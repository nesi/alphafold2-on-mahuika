{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#alphafold2-on-mahuika-cluster","title":"AlphaFold2 on Mahuika cluster","text":"<p>Description</p> <p>This package provides an implementation of the inference pipeline of AlphaFold v2.0. This is a completely new model that was entered in CASP14 and published in Nature. For simplicity, we refer to this model as AlphaFold throughout the rest of this document.</p> <p>Any publication that discloses findings arising from using this source code or the model parameters should cite the AlphaFold paper. Please also refer to the Supplementary Information for a detailed description of the method.</p> <p>Home page : https://github.com/deepmind/alphafold </p> <p>License and Disclaimer</p> <p>This is not an officially supported Google product.</p> <p>Copyright 2021 DeepMind Technologies Limited.</p> <p>AlphaFold Code License</p> <ul> <li> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at https://www.apache.org/licenses/LICENSE-2.0.</p> </li> <li> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> </li> </ul> <p>Model Parameters License</p> <ul> <li>The AlphaFold parameters are made available for non-commercial use only, under the terms of the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0) license. You can find details at: https://creativecommons.org/licenses/by-nc/4.0/legalcode</li> </ul> <p>References</p> <ul> <li>Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583\u2013589 (2021). https://doi.org/10.1038/s41586-021-03819-2</li> <li>Vlaams Instituut voor Biotechnologie, (2022). AlphaFold and Friends on the HPC. https://elearning.bits.vib.be/courses/alphafold/</li> <li>DeepMind. AlphaFold GitHub. https://github.com/deepmind/alphafold</li> </ul>"},{"location":"1_alphafold_pipeline/","title":"AlphaFold2 Pipeline","text":""},{"location":"1_alphafold_pipeline/#overview-of-the-architecture","title":"Overview of the Architecture","text":"<p>Developed at DeepMind, the AlphaFold architecture is a delicate collaboration between different modules, trained using end-to-end learning. This means that all parameters in the network are trained at once, from input to output, without the need of independently finetuning individual modules.</p> <p>The architecture is depicted as follows. Note that the visualizations below were copied and adapted from the official AlphaFold paper (Jumper et al, 2021).</p> <p></p> <p>A. User input</p> <p>The only input that the user needs to supply to AlphaFold in order to make a prediction, is a FASTA file with the protein primary sequence of interest. In the case of AlphaFold-Multimer, multiple proteins can be specified within the FASTA file.</p> <p>B. Database search</p> <p>For each sequence specified, a multiple sequence alignment (MSA) search is launched. This is done by first running JackHMMER on Mgnify (keeping the top 5\u2019000 matches) and UniRef90 (keeping the top 10\u2019000 matches), and then running HHBlits on UniClust30 + BFD (keeping all matches). </p> <p>As AlphaFold bases itself largely on coevolutionary information, a qualitative and deep MSA is essential for good predictions. In their publication, DeepMind claims that they see a significant drop in accuracy for MSAs of less than 30 sequences. In case of protein complexes, MSAs are paired according to evolutionary distance (prokaryotes) or simply the ranking of the matches (eukaryotes).</p> <p>Finally, although less important than MSA depth, a template search is done. Using the MSA obtained from UniRef90, PDB70 is searched with HHSearch, only allowing templates before a specified date to be found. After discarding templates identical to (a subset of) the input sequence, the top 4 templates are chosen. These templates serve as a starting position for the prediction models.</p> <p>C. Prediction model</p> <p>The MSA and templates are given to five AlphaFold models. These all have the same network architecture, but different parameters following five independent training stages with different randomization seeds. Thus, they will predict slightly different 3-D structures. </p> <p>The network architecture has two main parts. It consists of Evoformer blocks, which apply pairwise updates to numerical MSA representation and a 2-D pair representation, and Structure module blocks, which take care of the actual folding. These modules are repeated multiple times via a process called recycling, where the predicted 3-D structure is used as an input template for a new prediction iteration for further finetuning. By default, three recycling runs are done.</p> <p>D. Relaxation + output</p> <p>After each model predicts a 3-D structure, AMBER relaxation is done. Structures are ranked by the average predicted local distance difference test (pLDDT), found in the output of the prediction models. The pLDDT can be seen as a measure of local prediction confidence per position.</p> <p>To summarize, the following diagram illustrates the full prediction process: one database search is done to find MSAs and templates, and the exact same input is given to five identical neural network architectures, though parameterized differently. This yields five 3-D structures with tiny or big differences, which are optionally relaxed and finally ranked according to the model\u2019s confidence.</p> <p></p> <p>References</p> <p>Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583\u2013589 (2021). https://doi.org/10.1038/s41586-021-03819-2</p>"},{"location":"1_alphafold_pipeline/#alphafold2-training-scheme","title":"AlphaFold2 Training Scheme","text":"<p>To fully interpret AlphaFold predictions, it can be worthwhile to understand the training scheme behind it. Some interesting techniques are used at training time to improve model accuracy and the interpretability of results. Some of those techniques are discussed below.</p> <p>AlphaFold trains to accurately estimate a local and global confidence for the 3-D structure</p> <p>One of the most useful tools in the AlphaFold training procedure is the use of several losses. During training, any neural network modifies its parameters with a certain goal in mind. Typically, this goal is represented by a loss function that we want to minimize. In the case of AlphaFold, the loss function mainly tries to minimize the distance between the predicted 3-D structure and the actual PDB structure, which is called the Frame Aligned Point Error (FAPE). However, additional losses are also present:</p> <p>Distogram prediction loss Masked MSA prediction loss Structural violation loss (i.e. in the Structure module, there are no physical constraints imposed on the folding \u2013 the models implicitly learns these constraints using this loss) \u2026 Predicted local distance difference test loss (pLDDT) Predicted alignment error loss (PAE) The pLDDTs and PAEs allow us to directly derive a per-position confidence for predicted models. The pLDDT is even stored in the PDB files in output, in the b-factor column. The PAE can be (programatically) found in the .pkl files in output.</p> <p> In PyMol, setting color C &gt; spectrum &gt; b-factors allows us to visualize the local confidence (pLDDT), with in this case red being very confident, blue being very unconfident. In this visualization, the individual domains fold up rather nicely, with high confidence, and the linker parts are uncertain due to being disordered.</p> <p></p> <p> The global error prediction can be found using the PAE. This allows us to see if multi-domain chains fold properly. In the previous example, the pLDDT was high for both domains, but this plotted version of the PAE suggests that both domains are incorrectly located relative to the other in each of the five models.</p> <p>Training is done on protein fragments of max 256/348 residues</p> <p>Interestingly, AlphaFold is not trained on full proteins, but rather on protein fragments. In the first part of training, the maximum is set at 256 residues, and later on at 348. This is also the case for AlphaFold-Multimer, where the models are trained to correctly fold interacting proteins. In this case, appropriate subsequences are chosen to include a fair mix of interface and non-interface residues.</p> <p>Training is done partly on AlphaFold-predicted targets.</p> <p>Another key part of training is called self-distillation. Here, an initial model is trained on the PDB, after which it predicts 300\u2019000 structures from new sequences. These are then used together with the PDB structures in training the five final AlphaFold models. </p>"},{"location":"2_alphafold2_databases/","title":"AlphaFold2 Databases","text":"<p>AlphaFold databases are stored in /opt/nesi/db/alphafold_db/  parent directory. In order to make the database calling more convenient, we have prepared modules for each version of the database. Running <code>module spider AlphaFold2DB</code> will list the available versions named after the downladed Year and Month (Year-Month)</p> <p>terminal</p> <pre><code>$ module spider AlphaFold2DB\n\n---------------------------------------------\n  AlphaFold2DB:\n---------------------------------------------\n    Description:\n      AlphaFold2 databases\n\n     Versions:\n        AlphaFold2DB/2022-06\n        AlphaFold2DB/2023-04\n</code></pre> <p>Loading a module will set the <code>$AF2DB</code> environment variable which is pointing to the  selected version of the database. For an example. </p> <p>terminal</p> <pre><code>$ module load AlphaFold2DB/2023-04\n\n$ echo $AF2DB \n/opt/nesi/db/alphafold_db/2023-04\n</code></pre> <p>Databases and their sizes (<code>T</code> = TB , <code>G</code> = GB , <code>M</code> = MB)</p> <pre><code>2023-04/\n\u251c\u2500\u2500 bfd               1.8 T\n\u251c\u2500\u2500 mgnify            120 G\n\u251c\u2500\u2500 params            5.3 G\n\u251c\u2500\u2500 pdb70             56G G\n\u251c\u2500\u2500 pdb_mmcif         272 G\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mmcif_files\n\u251c\u2500\u2500 pdb_seqres        480 M\n\u251c\u2500\u2500 small_bfd          17 G\n\u251c\u2500\u2500 uniprot           111 G\n\u251c\u2500\u2500 uniref30          206 G\n\u2514\u2500\u2500 uniref90           73 G\n</code></pre> <ul> <li>Size of <code>2023-04</code> database is <code>~ 2.6 T</code></li> </ul>"},{"location":"3_slurmsubmit/","title":"Slurm submission scripts","text":"<p>Information on versions</p> <ul> <li>Following instructions are for AlphaFold &gt;= 2.3.2.</li> <li>If you are after an older version, refer to these page which the instruction on how to deploy it via Singularity/Apptainer containers </li> </ul> <p>Directory structure (recommended)</p> <ul> <li>Use nobackup filesystem at all times</li> <li>We recommend storing input .fasta files , corresponding outputs and slurm logs in three separate directories</li> </ul> <p></p>"},{"location":"3_slurmsubmit/#monomer","title":"<code>monomer</code>","text":""},{"location":"3_slurmsubmit/#slurm-script-for-a-single-monomer-query-file","title":"Slurm script for a single <code>monomer</code> query file","text":"<p>terminal</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      af-test\n#SBATCH --mem           24G\n#SBATCH --cpus-per-task 6\n#SBATCH --gpus-per-node A100:1\n#SBATCH --time          03:00:00\n#SBATCH --output        /nesi/nobackup/nesi12345/slurmlog/%j.out\n\nmodule purge\nmodule load AlphaFold2DB/2023-04\nmodule load AlphaFold/2.3.2\n\nINPUT_PATH=/nesi/nobackup/nesi12345/input\nOUTPUT=/nesi/nobackup/nesi12345/output\n\nrun_alphafold.py --use_gpu_relax \\\n--data_dir=$AF2DB \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2022_05.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniref30_database_path=$AF2DB/uniref30/UniRef30_2021_03 \\\n--pdb70_database_path=$AF2DB/pdb70/pdb70 \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--model_preset=monomer \\\n--max_template_date=2022-6-1 \\\n--db_preset=full_dbs \\\n--output_dir=${OUTPUT} \\\n--fasta_paths=${INPUT_PATH}/filename.fasta\n</code></pre>"},{"location":"3_slurmsubmit/#slurm-array-for-multiple-monomer-queries","title":"Slurm array for multiple <code>monomer</code> queries","text":"<p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      af--array_test\n#SBATCH --mem           24G\n#SBATCH --cpus-per-task 6\n#SBATCH --gpus-per-node A100:1\n#SBATCH --array         0-5\n#SBATCH --time          4:00:00\n#SBATCH --output        /nesi/nobackup/nesi12345/slurmlog/%A.%a.out\n\nmodule purge\nmodule load AlphaFold2DB/2023-04\nmodule load AlphaFold/2.3.2\n\nINPUT_PATH=/nesi/nobackup/nesi12345/input\nOUTPUT=/nesi/nobackup/nesi12345/output\n\nINPUT_FASTA=($INPUT_PATH/*.fasta)\nINPUT_NAMES=$(basename ${INPUT_FASTA[SLURM_ARRAY_TASK_ID]%.*})\n\nrun_alphafold.py --use_gpu_relax \\\n--data_dir=$AF2DB \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2022_05.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniref30_database_path=$AF2DB/uniref30/UniRef30_2021_03 \\\n--pdb70_database_path=$AF2DB/pdb70/pdb70 \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--model_preset=monomer \\\n--max_template_date=2022-6-1 \\\n--db_preset=full_dbs \\\n--output_dir=${OUTPUT}/${INPUT_NAMES} \\\n--fasta_paths=${INPUT_PATH}/${INPUT_NAMES}.fasta\n</code></pre>"},{"location":"3_slurmsubmit/#multimer","title":"<code>multimer</code>","text":""},{"location":"3_slurmsubmit/#slurm-script-for-a-single-multimer-query-file","title":"Slurm script for a single <code>multimer</code> query file\u00b6","text":"<p>code</p> <pre><code>#!/bin/bash -e\n\n#SBATCH --account       nesi12345\n#SBATCH --job-name      af-test\n#SBATCH --mem           24G\n#SBATCH --cpus-per-task 6\n#SBATCH --gpus-per-node A100:1\n#SBATCH --time          03:00:00\n#SBATCH --output        /nesi/nobackup/nesi12345/slurmlog/%j.out\n\nmodule purge\nmodule load AlphaFold2DB/2023-04\nmodule load AlphaFold/2.3.2\n\nINPUT_PATH=/nesi/nobackup/nesi12345/input\nOUTPUT=/nesi/nobackup/nesi12345/output\n\nrun_alphafold.py \\\n--use_gpu_relax \\\n--data_dir=$AF2DB \\\n--model_preset=multimer \\\n--uniprot_database_path=$AF2DB/uniprot/uniprot.fasta \\\n--uniref90_database_path=$AF2DB/uniref90/uniref90.fasta \\\n--mgnify_database_path=$AF2DB/mgnify/mgy_clusters_2022_05.fa \\\n--bfd_database_path=$AF2DB/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\\n--uniref30_database_path=$AF2DB/uniref30/UniRef30_2021_03 \\\n--pdb_seqres_database_path=$AF2DB/pdb_seqres/pdb_seqres.txt \\\n--template_mmcif_dir=$AF2DB/pdb_mmcif/mmcif_files \\\n--obsolete_pdbs_path=$AF2DB/pdb_mmcif/obsolete.dat \\\n--max_template_date=2022-6-1 \\\n--db_preset=full_dbs \\\n--output_dir=${OUTPUT} \\\n--fasta_paths=${INPUT}/test_multimer.fasta\n</code></pre>"},{"location":"4_alphafold_outputs/","title":"AlphaFold Outputs","text":"<p>Content of the output directory \"per\" query</p> <p>Each query will generate a subdirectory in output path with AlphaFold outputs. Below, a summary is given of their contents. The different file extensions are as follows:</p> <ul> <li><code>.pdb</code> \u2013 protein database format. These files contain the actual structures, including the pLDDT (local confidence) in the b-factor column</li> <li><code>.pkl</code> \u2013 pickle data format, used in programming, but in a binary format an thus not readable from the command line</li> <li><code>.json</code> \u2013 readable data format, often used in programming languages, but contents can be shown with cat and other commands msa files have .sto or .a3m file extensions, readable with cat and other commands</li> </ul> <p>Note that by default, there are no image files attached in the output, for instance visualizing the pairwise PTM scores (global confidence).</p> <p></p> <p>Running thousands of queries ?. Keep track of Disk and Inode quota</p> <ul> <li> <p>The files that take up the most space are usually the MSA (depending on the actual MSA size, but this can take several GBs) and the <code>.pkl</code> data files (grows quadratically with sequence length, with more than 1 GB reached for sequences of more than ~800 amino acids long)</p> </li> <li> <p>Use <code>nn_storage_quota</code> to keep track of usage </p> </li> </ul>"},{"location":"5_visualisation/","title":"Visualisation","text":"<p>jupyter</p> <ul> <li>Please use Mahuika JupyterHub for Visualisation component. JupyterHub can be accessed via https://jupyter.nesi.org.nz/hub/login</li> </ul>"},{"location":"5_visualisation/#visualisation-script","title":"Visualisation Script","text":"<p>We provide a modified version of a python script (<code>visualize_alphafold_results.py</code>) created by VIB Belgium to extract pLDDT, PAE and MSA visualisations (inspired by ColabFold code). </p> <ul> <li>This script is incorporated into <code>pymol-open-source/2.5.0</code> module The script uses the contents of the <code>.pkl</code> output files from the AlphaFold run. It takes three parameters in input:</li> <li><code>--input_dir</code> The location where the AlphaFold output files were stored.</li> <li><code>--output_dir</code> (optional) The location where the images that are generated should be stored. By default, they are stored in the input directory.</li> <li><code>--name</code> (optional) The prefix that will be used in for the filenames of the generated files. By default, no prefix is added. To run the script, you will also have the correct python modules loaded. You can do this by running the following lines before running the actual script.</li> </ul> <p>code</p> <ul> <li>Content of the directory with results from AlphaFold run. This will be the value for <code>--input_dir</code> for <code>visualize_alphafold_results.py</code> <pre><code>$ ls -F GA98/\nfeatures.pkl  ranked_2.pdb        result_model_1_ptm_pred_0.pkl  result_model_5_ptm_pred_0.pkl     unrelaxed_model_3_ptm_pred_0.pdb\nmsas/         ranked_3.pdb        result_model_2_ptm_pred_0.pkl  timings.json                      unrelaxed_model_4_ptm_pred_0.pdb\nranked_0.pdb  ranked_4.pdb        result_model_3_ptm_pred_0.pkl  unrelaxed_model_1_ptm_pred_0.pdb  unrelaxed_model_5_ptm_pred_0.pdb\nranked_1.pdb  ranking_debug.json  result_model_4_ptm_pred_0.pkl  unrelaxed_model_2_ptm_pred_0.pdb\n</code></pre></li> <li>Script can be used as below <pre><code>$ module purge\n\n$ module load pymol-open-source/2.5.0\n\n$ mkdir vis_outputs\n\n$ visualize_alphafold_results.py --input_dir GA98 --output_dir vis_outputs --name GA98\n</code></pre></li> <li>This should generate two .png files <pre><code>$ ls vis_outputs/\n  GA98_coverage_LDDT.png  GA98_PAE.png\n</code></pre></li> </ul> <p>Content of <code>GA98_coverage_LDDT.png</code> </p> <p></p> <p>Content of <code>GA98_PAE.png</code></p> <p></p>"},{"location":"5_visualisation/#using-pymol-molecular-visualization-system-via-virtual-desktop","title":"Using PyMOL molecular visualization system via Virtual Desktop","text":"<p>Launch <code>pymol</code></p> <ol> <li> <p>Click  Virtual Desktop icon    </p> </li> <li> <p>Open a Terminal session on Virtual Desktop and execute <pre><code>$ module purge\n\n$ module load pymol-open-source/2.5.0\n\n$ pymol\n</code></pre></p> </li> </ol> <p></p>"},{"location":"6_supp1_runtime/","title":"Supplementary. 1:  Benchmarking","text":"<p>export</p> <ul> <li>.fasta amino acid sequence files used for benchmarking can be found here<ul> <li>source https://www.rcsb.org/</li> </ul> </li> <li>Below is the composition of amino acids per Protein used in benchmarking. <ul> <li>These were counted with.     <pre><code>cat NAME.fasta | grep -v \"&gt;\" | fold -w1 | sort | uniq -c\n</code></pre></li> </ul> </li> </ul> Amino acid sequences  Amino Acid 1M6I 1Q7F 1YOG 2D6C 3MZS 3RGK 5CWV 5HB5 5HB7 6V3H 7N4X A 41 12 17 17 23 13 45 8 15 119 124 C 3 6 2 7 1 41 41 D 27 15 7 7 24 8 15 7 5 54 50 E 36 13 14 14 34 14 18 8 13 60 52 F 18 17 6 6 30 7 11 6 2 83 75 G 46 26 11 11 23 15 23 12 6 75 86 H 9 10 12 12 19 9 8 4 6 31 20 I 32 26 9 9 31 8 17 8 6 61 61 K 32 16 19 19 33 19 8 6 5 43 37 L 38 14 18 18 51 17 45 9 5 171 186 M 8 6 2 2 15 3 4 5 2 24 30 N 17 21 1 1 20 3 13 6 3 60 57 P 27 7 4 4 30 5 16 12 11 81 71 Q 15 20 5 5 19 7 17 3 6 50 54 R 29 16 4 4 29 3 20 11 11 58 55 S 34 12 6 6 22 7 32 9 6 111 112 T 22 10 5 5 23 4 26 8 4 68 70 V 42 28 8 8 29 7 28 12 8 88 91 W 7 2 2 9 2 1 3 1 12 16 Y 10 11 3 3 20 2 10 3 9 46 44 <p>Runtimes on A100 GPUs and Memory (GB) utilisation</p> <p></p> <p></p> <p></p> <p> I/O bottleneck</p> <ul> <li>notes pending..</li> </ul>"},{"location":"7_supp2_troubleshoot/","title":"Supplementary. 2: Troubleshooting","text":"<p>1. <code>RuntimeError: Resource exhausted: Out of memory</code></p> <ul> <li>If you are to encounter the message \"RuntimeError: Resource exhausted: Out of memory\" , add the following variables to the slurm script</li> <li>For module based runs  ( version <code>2.3.2</code> and above)   <pre><code>export TF_FORCE_UNIFIED_MEMORY=1\nexport XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n</code></pre></li> <li>For Singularity container based runs   <pre><code>export SINGULARITYENV_TF_FORCE_UNIFIED_MEMORY=1 \nexport SINGULARITYENV_XLA_PYTHON_CLIENT_MEM_FRACTION=4.0\n</code></pre></li> </ul> <p>2. <code>ValueError: Invalid character in the sequence: n</code></p> <p>This will get triggered in the event where an input file doesn't obey the following conditions</p> <ul> <li>AlphaFold expects an input query sequence with capitalized one-letter amino-acid types from this set:</li> </ul> <pre><code> 'A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', \n 'S', 'T', 'W', 'Y', 'V' \n</code></pre> <ul> <li>For <code>monomer</code> runs: Input file for the AlphaFold system should only contain one protein.</li> </ul>"}]}